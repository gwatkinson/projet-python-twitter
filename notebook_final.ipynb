{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Projet de programmation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Introduction <a name=\"intro\"></a>\n",
    "\n",
    "\n",
    "Ceci est notre projet d'informatique de 2<sup>ème</sup> année de l'ENSAE pour le cours *Python pour un data scientist* où nous avons choisis d'utiliser l'**API de Twitter** et du **sentimental analysis**.\n",
    "\n",
    "**Problématique** : $\\underline{​​\\text{​​ En quoi Twitter reflète-t-il la polarisation aux États-Unis, autour des élections présidentielles Américaines ?}​​}​​$\n",
    "\n",
    "### Installation\n",
    "\n",
    "Récupération du projet sur GitHub avec :\n",
    "\n",
    "        git clone https://github.com/gwatkinson/projet-python-twitter.git\n",
    "        cd projet-python-twitter\n",
    "\n",
    "Upgrade pip :\n",
    "\n",
    "        python3 -m pip install --upgrade pip\n",
    "\n",
    "Créer un environnement virtuel :\n",
    "\n",
    "        pip install virtualenv\n",
    "        virtualenv .venv\n",
    "        source .venv/bin/activate\n",
    "        # .venv\\Scripts\\activate # sur Windows\n",
    "\n",
    "Pour utiliser directement le `setup.py` :\n",
    "\n",
    "        pip install .\n",
    "\n",
    "Sinon, installer directement des packages nécessaire :\n",
    "\n",
    "        pip install -r requirements.txt\n",
    "\n",
    "\n",
    "Puis, il faut créer le fichier `projet/_credentials.py`, qui contient les clés de l'API de Twitter.\n",
    "\n",
    "Dans le format suivant :\n",
    "\n",
    "```python\n",
    "credentials = {\n",
    "    \"consumer_key\": \"XXXXXXX\",\n",
    "    \"consumer_secret\": \"XXXXXXX\",\n",
    "    \"access_token\": \"XXXXXXX\",\n",
    "    \"access_token_secret\": \"XXXXXXX\",\n",
    "}\n",
    "```\n",
    "\n",
    "Finalement, il faut ajouter les données dans le dossier `data/json/`.\n",
    "\n",
    "### Modules\n",
    "\n",
    "* <a href=\"https://gwatkinson.github.io/projet-python-twitter/projet/streaming.html\" target=\"_blank\">streaming</a>\n",
    "* <a href=\"https://gwatkinson.github.io/projet-python-twitter/projet/processing.html\" target=\"_blank\">processing</a>\n",
    "* <a href=\"https://gwatkinson.github.io/projet-python-twitter/projet/modelisation.html\" target=\"_blank\">modelisation</a>\n",
    "* <a href=\"https://gwatkinson.github.io/projet-python-twitter/projet/visualisation.html\" target=\"_blank\">visualisation</a>\n",
    "\n",
    "### Documentation\n",
    "\n",
    "<a href=\"https://gwatkinson.github.io/projet-python-twitter/\" target=\"_blank\">https://gwatkinson.github.io/projet-python-twitter/</a>\n",
    "\n",
    "### Auteurs\n",
    "\n",
    "* Gabriel Watkinson\n",
    "* Mathias Vigouroux\n",
    "* Wilfried Yapi"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Table des matières\n",
    "\n",
    "* [Introduction](#intro)\n",
    "* [1)Récupération des données](#data)\n",
    "* [2)Modélisation](#model)\n",
    "    * [a.Prepocessing](#process)\n",
    "    * [b.Clustering](#cluster)\n",
    "* [3)Visualisation](#visu)\n",
    "    * [a.Table des États](#states)\n",
    "    * [b.Carte interactive](#map)\n",
    "* [Conclusion](#conc)\n",
    "* [Annexes](#annex)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1) Récupération des données <a name=\"data\"></a>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Nous avons utilisé l'**API** de Twitter pour récupérer les nouveaux tweets publiés sur Twitter, la nuit du 3 au 4 Novembre 2020 (la nuit de l'éléction américaine). Nous avons seulement récupérer les tweets qui contennaient certains mots :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```python\n",
    "# Liste 3 sur Trump et Biden uniquement\n",
    "liste_3 = [\n",
    "    \"biden\",\n",
    "    \"trump\",\n",
    "    \"JoeBiden\",\n",
    "    \"realDonaldTrump\",\n",
    "]\n",
    "\n",
    "# Liste 4 sur le thème 'vote'\n",
    "liste_4 = [\n",
    "    \"iwillvote\",\n",
    "    \"govote\",\n",
    "    \"uselection\",\n",
    "    \"vote\",\n",
    "]\n",
    "\n",
    "# Liste 5 sur le thème 'election'\n",
    "liste_5 = [\n",
    "    \"uselection\",\n",
    "    \"president\",\n",
    "    \"presidentialelection\",\n",
    "    \"presidential\",\n",
    "    \"electionnight\",\n",
    "]\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Pour cela, nous avons utilisé le module python `tweepy` ainsi que les fonctions codées dans le module [streaming](https://gwatkinson.github.io/projet-python-twitter/projet/streaming.html) (voir la documentation pour plus d'information sur [start_stream](https://gwatkinson.github.io/projet-python-twitter/projet/streaming.html#projet.streaming.start_stream)). Voici un exemple d'utilisation du code que nous avons écrit :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projet.streaming as stream                           # Contient les fonctions pour le streaming\n",
    "import projet.listes_mots as listes                         # Contient les listes de mots\n",
    "import projet._credentials as cred                          # Contient les clés d'authentification à l'API\n",
    "\n",
    "credentials = stream.CredentialsClass(cred.credentials)     # Pour se connecter à l'API (il faut le fichier projet/_credentials.py)\n",
    "\n",
    "stream.start_stream(\n",
    "    credentials=credentials,\n",
    "    liste_mots=listes.liste_3,                              # Liste des mots à tracker (voir `projet.listes_mots`)\n",
    "    nb=200,                                                 # Nombre de tweets à recupérer\n",
    "    # timeout=10/3600,                                        # Durée du stream\n",
    "    fprefix=\"exemple_liste_3\",                              # À modifier en fonction de la liste selectionnée\n",
    "    path=\"./data/\",                                         # À modifier selon l'utilisateur (doit finir par \"/\" ou \"\\\")\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "source": [
    "Un fichier du format `exemple_liste_3_{date}.json` a été créé dans `data/`.\n",
    "\n",
    "Pour voir à quoi ressemble les données :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "path = glob.glob(\"data/exemple_liste_3*.json\")[-1]  # On récupère le dernier fichier exemple crée\n",
    "print(\"On regarde le fichier : \"+path+\"\\n\")\n",
    "\n",
    "tweets_list = []\n",
    "with open(path, \"r\") as fh:\n",
    "    file = fh.read().split(\"\\n\")\n",
    "    for line in file:\n",
    "        if line:\n",
    "            tweets_list.append(json.loads(line))\n",
    "\n",
    "print(\"Le premier tweet :\")\n",
    "print(tweets_list[0])\n"
   ]
  },
  {
   "source": [
    "Il s'agit du format `json`. Il est difficile de voir les variable comme cela. On peut créer une `dataframe pandas` pour mieux comprendre les données."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.DataFrame(tweets_list)\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"On voit bien qu'il y a {df_tweets.shape[0]} lignes (une par tweet) et {df_tweets.shape[1]} colonnes (en fait, il y a plus de variables car certaines colonnes sont des dictionnaires).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Colonnes :\\n\")\n",
    "for name in list(df_tweets):\n",
    "    print(name)"
   ]
  },
  {
   "source": [
    "Mais certaine variables sont des dictionnaires (par exemple : `user`, `place`, ...), et il faut donc nettoyer un petit peu la dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2) Modélisation <a name=\"model\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### a. Preprocessing <a name=\"process\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Nous avons fait une fonction qui fait les étapes précédentes ainsi que des fonctions pour nettoyer les données. Elles sont dans le fichier [processing](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projet.processing as process     # Contient les fonctions pour le processing de la dataframe"
   ]
  },
  {
   "source": [
    "Voir [tweet_json_to_df](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.tweet_json_to_df) pour plus de détails."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "paths = glob.glob(\"data/json/*.json\")\n",
    "list_path = random.sample(paths, int(len(paths)/3))                      # On récupère un tiers des fichiers json dans le dossier 'data/json/'\n",
    "# folder = \"./data/json/\"                                                   # Pour récupèrer tous les fichiers json dans le dossier 'data/json/'\n",
    "\n",
    "full_df = process.tweet_json_to_df(path_list=list_path, verbose=True)    # Convertit les json en dataframe pandas\n"
   ]
  },
  {
   "source": [
    "Nous avons ainsi récupérer les fichiers dans `data/json/` dans la dataframe pandas `full_df`.\n",
    "\n",
    "Elle ressemble à :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "full_df.head()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "print(f\"On a ainsi récupèrer {full_df.shape[0]} tweets et {full_df.shape[1]} colonnes (il y a en fait plus de variables car certaines colonnes sont des dictionnaires).\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "On peut ensuite utiliser `clean_df()` pour filtrer et nettoyer la base de donnée en conservant seulement les informations qui nous interressent. On peut aussi utiliser une liste de `listes_variables` pour récupérer d'autres variables. \n",
    "\n",
    "Voir la doc pour plus de détails : [clean_df](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.clean_df).\n",
    "\n",
    "On créée ainsi la dataframe finale que l'on nomme `df` :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projet.listes_variables  # Liste des variables à selectionner\n",
    "\n",
    "df = process.clean_df(full_df, index=\"id\", date=\"created_at\", verbose=True, columns=projet.listes_variables.liste_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "Il reste 24 colonnes au lieu de 37. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "On récupère le texte entier, qui se trouve la colonne `extended_tweet-full_text` ou dans `retweeted_status-extended_tweet-full_text`, avec la fonction [get_full_text](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.get_full_text) qui ajoute la colonne `full_text`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.get_full_text(\n",
    "    df,\n",
    "    new_var=\"full_text\",\n",
    "    text_vars=[\n",
    "        \"extended_tweet-full_text\",\n",
    "        \"retweeted_status-extended_tweet-full_text\",\n",
    "        \"retweeted_status-text\",\n",
    "        \"text\",\n",
    "    ],\n",
    "    drop_vars=True,  # drop_vars=True supprime les anciennes colonnes contenant du texte\n",
    ")\n",
    "\n",
    "df[\"full_text\"].head()"
   ]
  },
  {
   "source": [
    "Ensuite, on ajoute des colonnes (`full_text-contains_trump`, `full_text-contains_biden`, `user-description-contains_biden` et `user-description-contains_biden`) qui indique la présence de Trump ou de Biden dans le texte ou la description de l'utilisateur. Voir [add_politics](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.add_politics)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.add_politics(\n",
    "    df,\n",
    "    trump_word=\"(Trump|Donald|realDonaldTrump|republican)\", # Mot pour considerer la présence de Trump\n",
    "    biden_word=\"(Biden|Joe|JoeBiden|democrat)\", # Pour biden\n",
    "    case=False, # case sensitive\n",
    "    trump_var=\"contains_trump\",\n",
    "    biden_var=\"contains_biden\",\n",
    "    text_vars=[\"full_text\", \"user-description\"],\n",
    ")\n",
    "\n",
    "df[[\"full_text\", \"full_text-contains_trump\", \"full_text-contains_biden\"]].head()"
   ]
  },
  {
   "source": [
    "En utilisant `SentimentIntensityAnalyzer` du module `sentiment.vader` de la librairie `nltk`, on ajoute les colonnes qui contiennent le sentiment compound (compris entre -1 et 1) du `full_text` et de `user-description`. Voir [add_sentiment](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.add_sentiment)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.add_sentiment(\n",
    "    df,\n",
    "    text_vars=[\"full_text\", \"user-description\"],\n",
    "    sent_var=\"sentiment\",\n",
    "    compound_var=\"compound\",\n",
    "    keep_dict=False,\n",
    ")\n",
    "\n",
    "df[[\"full_text\", \"full_text-sentiment-compound\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lang\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pourcentage de tweet non anglais : \", round(100*len(df[df[\"lang\"]!=\"en\"])/len(df)), \"%\")"
   ]
  },
  {
   "source": [
    "On voit qu'il y a un pourcentage important de tweets étrangers.\n",
    "\n",
    "On conserve seulement les tweets en anglais. Voir [keep_lang](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.keep_lang)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = process.keep_lang(df, lang_var=\"lang\", language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5), sharey=True)\n",
    "\n",
    "axs[0].hist(df_en[\"full_text-sentiment-compound\"], color=\"g\", bins=20)\n",
    "axs[1].hist(df_en[df_en[\"full_text-contains_trump\"] & ~df_en[\"full_text-contains_biden\"]][\"full_text-sentiment-compound\"], color=\"r\", bins=20)\n",
    "axs[2].hist(df_en[~df_en[\"full_text-contains_trump\"] & df_en[\"full_text-contains_biden\"]][\"full_text-sentiment-compound\"], color=\"b\", bins=20)\n",
    "fig.suptitle(\"Histogramme du sentiment compound de full_text\")\n",
    "axs[0].set_title(\"Tous les tweets\", color=\"g\")\n",
    "axs[1].set_title(\"Contenant Trump\", color=\"r\")\n",
    "axs[2].set_title(\"Contenant Biden\", color=\"b\")\n",
    "plt.show()\n",
    "fig.savefig(\"image/maps/stats_desc_old.jpg\")"
   ]
  },
  {
   "source": [
    "On remarque qu'il y a beaucoup de compound nuls. Cela peut être du au fait qu'il y a un nombre important de tweets très courts (émojis, numerique).\n",
    "\n",
    "On les enlève pour mieux voir la polarisation (voir [remove_null](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.remove_null)) :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null = process.remove_null(df_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5), sharey=True)\n",
    "\n",
    "axs[0].hist(df_null[\"full_text-sentiment-compound\"], color=\"g\", bins=20)\n",
    "axs[1].hist(df_null[df_null[\"full_text-contains_trump\"] & ~df_null[\"full_text-contains_biden\"]][\"full_text-sentiment-compound\"], color=\"r\", bins=20)\n",
    "axs[2].hist(df_null[~df_null[\"full_text-contains_trump\"] & df_null[\"full_text-contains_biden\"]][\"full_text-sentiment-compound\"], color=\"b\", bins=20)\n",
    "fig.suptitle(\"Histogramme du sentiment compound de full_text\")\n",
    "axs[0].set_title(\"Tous les tweets\", color=\"g\")\n",
    "axs[1].set_title(\"Contenant Trump\", color=\"r\")\n",
    "axs[2].set_title(\"Contenant Biden\", color=\"b\")\n",
    "plt.show()\n",
    "fig.savefig(\"image/maps/stats_desc.jpg\")"
   ]
  },
  {
   "source": [
    "On peut ensuite rajouter un label aux compounds pour les discrétiser. Voir [sentiment_class](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.sentiment_class)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.sentiment_class(\n",
    "    df_null,\n",
    "    categories=[\n",
    "        (\"tneg\", -1, -0.7),\n",
    "        (\"neg\", -0.7, -0.2),\n",
    "        (\"neutre\", -0.2, 0.2),\n",
    "        (\"pos\", 0.2, 0.7),\n",
    "        (\"tpos\", 0.7, 1),\n",
    "    ],\n",
    "    compound_vars=[\n",
    "        \"full_text-sentiment-compound\",\n",
    "        \"user-description-sentiment-compound\",\n",
    "    ],\n",
    "    class_var=\"class\",\n",
    ")\n",
    "\n",
    "df_null[[\"full_text-sentiment-compound\", \"full_text-sentiment-compound-class\"]].head()"
   ]
  },
  {
   "source": [
    "On ajoute ensuite un label qui joint le sentiment du tweet et la présence de Biden ou Trump. Voir [add_label](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.add_label)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.add_label(\n",
    "    df_null,\n",
    "    label_var=\"label\",\n",
    "    trump_var=(\"full_text-contains_trump\", \"T\"),\n",
    "    biden_var=(\"full_text-contains_biden\", \"B\"),\n",
    "    missing_var=\"N\",\n",
    "    class_var=\"full_text-sentiment-compound-class\",\n",
    ")\n",
    "\n",
    "df_null[[\"full_text-contains_trump\", \"full_text-contains_biden\", \"full_text-sentiment-compound-class\", \"label\"]].head()"
   ]
  },
  {
   "source": [
    "Finalement, on peut ajouter les États à partir de la description de la localisation fournie par les utilisateurs.\n",
    "\n",
    "On a essayé trois manières de les obtenir :\n",
    "\n",
    "* Utiliser les coordonnées données par la variable `place` du tweet pour determiner l'état. Cependant, moins de 1% des tweets possèdent cette variable.\n",
    "\n",
    "* Utiliser une librairie de NLP pour reconnaître la présence d'État ou d'une ville dans la chaîne de caractère données par l'utilisateur (`user-location`). Cependant, la librairie que l'on a trouvé est lente et pas significativement plus efficace que la troisième méthode qui est plus simple. Voir la fonction [get_states1](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.get_states1) pour plus de détails.\n",
    "\n",
    "* Finalement, on a décider d'utiliser des expressions régulières pour identifier le nom des États. Voir [get_states](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.get_states).\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = df_null[\"place-bounding_box-coordinates\"].count() / len(df_null)\n",
    "print(f\"Il y a{pc*100: .1f}% de tweets qui donnent leur position avec la première méthode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.get_states(df_null, state_var=\"state\", location_var=\"user-location\")\n",
    "\n",
    "df_null[[\"user-location\", \"state\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc2 = df_null[\"state\"].count() / len(df_null)\n",
    "print(f\"Il y a{pc2*100: .1f}% de tweets qui donnent leur position avec la troisième méthode.\")"
   ]
  },
  {
   "source": [
    "On conserve uniquement les tweets avec une position (voir [keep_states](https://gwatkinson.github.io/projet-python-twitter/projet/processing.html#projet.processing.keep_states)):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = process.keep_states(df_null, state_var=\"state\")\n",
    "\n",
    "df_final[[\"user-location\", \"state\"]].head()"
   ]
  },
  {
   "source": [
    "n_init = len(full_df)\n",
    "n_final = len(df_final)\n",
    "\n",
    "print(f\"Il reste {n_final} des {n_init} tweets initiaux après le nettoyage de la dataframe.\")\n",
    "print(f\"Soit {100*n_final/n_init :.1f}%.\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### b. Clustering <a name=\"cluster\"></a>\n",
    "\n",
    "On passe à la partie sur le clustering."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projet.modelisation as model     # Contient les fonctions pour le clustering"
   ]
  },
  {
   "source": [
    "On fait du K-means. On standardise une copie de la df puis ajoute une colonne label à df principale.\n",
    "\n",
    "On le fait deux fois :\n",
    "* La première utilise la méthode du coude pour déterminer le nombre de clusters et utilise toutes les variables numériques (sauf l'id).\n",
    "\n",
    "* La seconde a un nombre de clusters détérminés (6), et utilise seulement quelques variables.\n",
    "\n",
    "Voir la doc pour plus de détails : [KM](https://gwatkinson.github.io/projet-python-twitter/projet/modelisation.html#projet.modelisation.KM)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.KM(\n",
    "    df_final,\n",
    "    n_cluster=None,             # Prend la valeur optimal par la méthode du coude\n",
    "    label_var=\"kmlabel_opt\",    # Nom de la nouvelle colonne\n",
    "    vars=None,                  # Prend les valeurs numériques et booléennes sans l'id\n",
    "    drop_vars=[\"user-id\"],\n",
    "    n_init=10,                  # Nombre de fois que le k-means est lancé\n",
    "    max_iter=300,               # Nombre max d'itérations\n",
    "    max_cluster=10,             # Nombre max de clusters\n",
    "    random_state=20,\n",
    "    plot=True,                  # Plot le SSE\n",
    ")\n",
    "\n",
    "print()"
   ]
  },
  {
   "source": [
    "<a name=\"kmclusters\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.groupby([\"kmlabel_opt\", \"label\"]).describe()[\"user-id\"][\"count\"]"
   ]
  },
  {
   "source": [
    "On voit au dessus le nombre de tweets par label selon le cluster détérminé par le 1<sup>er</sup> kmeans.\n",
    "\n",
    "Cette répartition ne correspond pas au sentiment du tweet selon la présence de Trump ou Biden, notamment parcequ'on prend en compte la popularité des utilisateurs (nombre de followers, d'amis, ...).\n",
    "\n",
    "Donc on considère un autre modèle avec moins de variables :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.KM(\n",
    "    df_final,\n",
    "    n_cluster=6,                                # 6 clusters\n",
    "    label_var=\"kmlabel\",                        # Nom de la nouvelle colonne\n",
    "    vars=[\n",
    "        'full_text-contains_trump',             # Selection des variables pour le clusters\n",
    "        'full_text-contains_biden',\n",
    "        'full_text-sentiment-compound',\n",
    "        'user-description-sentiment-compound'\n",
    "    ],\n",
    "    n_init=10,                                  # Nombre de fois que le k-means est lancé\n",
    "    max_iter=300,                               # Nombre max d'itérations\n",
    "    random_state=30,\n",
    ")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[\"kmlabel\"].value_counts().plot(kind=\"hist\", title=\"Histogramme des labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.groupby([\"kmlabel\", \"label\"]).describe()[\"user-id\"][\"count\"]"
   ]
  },
  {
   "source": [
    "On peut identifier les clusters à des groupes de tweets homogènes d'un point de vue du sentiment politique.\n",
    "\n",
    "Par exemple, un cluster des tweets qui supportent Trump, ceux qui supportent Biden, ceux qui haïssent Trump, ect..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3) Visualisation <a name=\"visu\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projet.visualisation as visu     # Contient les fonctions pour la visualisation"
   ]
  },
  {
   "source": [
    "### a. Table des États <a name=\"states\"></a>\n",
    "\n",
    "On créée une dataframe geopandas qui contient la forme des États américains (voir [create_gdf](https://gwatkinson.github.io/projet-python-twitter/projet/visualisation.html#projet.visualisation.keep_states)) :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = visu.create_gdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "source": [
    "### b. Carte interactive <a name=\"map\"></a>\n",
    "\n",
    "On créée les images des histogrammes par État dans `images/`. Voir [save_hist](https://gwatkinson.github.io/projet-python-twitter/projet/visualisation.html#projet.visualisation.keep_states).\n",
    "\n",
    "On ajoute le cluster majoritaire dans chaque État. Voir [add_max](https://gwatkinson.github.io/projet-python-twitter/projet/visualisation.html#projet.visualisation.add_max).\n",
    "\n",
    "Finalement, on ajoute des stats (total, moyenne, écart-type) du sentiment compound par État."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "visu.save_hist(df_final, gdf, label=\"kmlabel\")  # Ajoute les histogrammes dans le dossier image/hist/kmlabel\n",
    "visu.save_hist(df_final, gdf, label=\"label\")\n",
    "visu.add_max(df_final, gdf, label=\"label\")      # Ajoute le cluster majoritaire de chaque état\n",
    "visu.add_max(df_final, gdf, label=\"kmlabel\")\n",
    "gdf2 = visu.add_stats_sentiment(df_final, gdf)         # Ajoute des stats sur le sentiment compound du full_text\n",
    "\n",
    "gdf2.head()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "On affiche les stats par État :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axs2 = plt.subplots(1, 3, figsize=(24,7))\n",
    "fig2.suptitle(\"Stats du sentiment compound par État\")\n",
    "mask = (gdf2[\"NAME10\"]!=\"Alaska\") & (gdf2[\"NAME10\"]!=\"Hawaii\")\n",
    "gdf2[mask].plot(column=\"count\", legend=True, ax=axs2[0], legend_kwds={'label': \"Nombre de tweets par État\", 'orientation': \"horizontal\"})\n",
    "gdf2[mask].plot(column=\"mean\", legend=True, ax=axs2[1], legend_kwds={'label': \"Moyenne du sentiment compound par État\", 'orientation': \"horizontal\"})\n",
    "gdf2[mask].plot(column=\"std\", legend=True, ax=axs2[2], legend_kwds={'label': \"Écart-type du sentiment compound par État\", 'orientation': \"horizontal\"})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2.savefig(\"image/maps/stats.jpg\")"
   ]
  },
  {
   "source": [
    "Les couleurs désignent le cluster majoritaire dans l'État. Voir [les clusters](#kmclusters).\n",
    "\n",
    "On affiche les cartes interactives (voir [plot_hist](https://gwatkinson.github.io/projet-python-twitter/projet/visualisation.html#projet.visualisation.plot_hist)):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu.plot_hist(gdf2, label=\"kmlabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu.plot_hist(gdf2, label=\"label\")"
   ]
  },
  {
   "source": [
    "## Conclusion <a name=\"conc\"></a>\n",
    "\n",
    "Grâce au cartes, on visualise bien la polarisation des États-Unis, notamment autour du sujet des éléctions américaines et des candidats Donald Trump et Joe Biden. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}